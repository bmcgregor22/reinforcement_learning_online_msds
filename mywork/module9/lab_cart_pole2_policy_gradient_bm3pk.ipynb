{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GBod1nv65dN"
      },
      "source": [
        "## Lab: Cart Pole Demo 2 using OpenAI gym\n",
        "## Policy Gradient\n",
        "\n",
        "### University of Virginia\n",
        "### Reinforcement Learning\n",
        "#### Last updated: March 4, 2024\n",
        "\n",
        "#### Bruce McGregor (BM3PK)\n",
        "#### November 14, 2025\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J73-oPOTqC5b"
      },
      "source": [
        "#### Instructions:  \n",
        "\n",
        "Carefully read the notes below and run the provided code. Answer each question clearly and show all results.\n",
        "\n",
        "#### TOTAL POINTS: 12\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6__XiTWDIiT"
      },
      "source": [
        "### First a Refresh\n",
        "\n",
        "Let's briefly review the content from Demo 1: Basics and Simple Policy\n",
        "\n",
        "We revisit the CartPole problem.\n",
        "\n",
        "We will work with the fork [gymnasium](https://gymnasium.farama.org/) which maintains OpenAI gym.  \n",
        "\n",
        "The *simple policy* didn't perform very well: the average reward was about 42.\n",
        "\n",
        "We want to see if we can do better using a Policy Gradient algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA_dd0h6Eoap"
      },
      "source": [
        "### Setup and First Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wM5RiqAeqHJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8842f8-11cd-4139-ea89-c4e29ca20846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M4HpQPsAqH4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c018707-a99a-48db-fcbe-907efcddce72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting renderlab\n",
            "  Downloading renderlab-0.1.20230421184216-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.0.3)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (0.0.4)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy->renderlab) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2025.10.5)\n",
            "Downloading renderlab-0.1.20230421184216-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: renderlab\n",
            "Successfully installed renderlab-0.1.20230421184216\n"
          ]
        }
      ],
      "source": [
        "! pip install renderlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ed_DyvHy6pbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff001233-073b-4a7a-b821-049ec8e88dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import renderlab as rl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA4JgtuL5jC9"
      },
      "source": [
        "Load the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UcGLRi6F6vcQ"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
        "state = env.reset(seed=314)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtQFJsxj7aBj"
      },
      "source": [
        "Given the state, we take an action. The next state comes from the environment, which is encoded in `gym`.\n",
        "\n",
        "Components:   \n",
        "[0]: cart horizontal position (0.0 = center)  \n",
        "[1]: velocity (positive means right)  \n",
        "[2]: angle of the pole (0.0 = vertical)  \n",
        "[3]: pole's angular velocity (positive means clockwise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M30S6Bf_6yNv",
        "outputId": "a9d061cb-b29b-4222-fbca-a4653d83797e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.04225422, 0.02126478, 0.02520455, 0.00700802], dtype=float32), {})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A5peNKiBXsh",
        "outputId": "7c9e55d9-3b99-4261-d058-b324796ade6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# state space number of components\n",
        "env.observation_space.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GkZmsIN7ky6"
      },
      "source": [
        "The action space consists of two options:\n",
        "\n",
        "[0]: move cart left   \n",
        "[1]: move cart right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySQ8EpsV7Ngr",
        "outputId": "d1ce6b6e-60ec-44bd-dc01-287dbaebc270"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_68lpUuVCc-_"
      },
      "source": [
        "Let's take an action, draw a sample and look at the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKFEIHjL7n7h",
        "outputId": "4cf2c923-e20e-4fcc-d8a7-5051c3c8f053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state [ 0.04267951  0.21601637  0.02534471 -0.27761722]\n",
            "reward 1.0\n",
            "done False\n",
            "info {}\n"
          ]
        }
      ],
      "source": [
        "# move right\n",
        "action = 1\n",
        "\n",
        "# take a step and get next state, reward from environment\n",
        "state, reward, terminated, truncated, info = env.step(action)\n",
        "done = terminated or truncated\n",
        "\n",
        "print('state', state)\n",
        "print('reward', reward)\n",
        "print('done', done)\n",
        "print('info', info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqxI0vej88Kh"
      },
      "source": [
        "**Reward and Episode**  \n",
        "\n",
        "For each time step that the cart keeps the pole balanced, it earns reward 1.\n",
        "\n",
        "If the pole tilts too much or if the cart moves off screen, `reward=0` and `done=True` (the episode will end).\n",
        "\n",
        "When the episode ends, a new episode may begin. The process learns cumulatively from each episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv5gKVde9pvQ"
      },
      "source": [
        "**Simple policy**:  \n",
        "\n",
        "When the pole leans left (negative angle), move left. When the pole leans right (positive angle), move right.\n",
        "\n",
        "Run many episodes and visualize their reward distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dREsiRKf9CbR"
      },
      "outputs": [],
      "source": [
        "def simple_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "num_episodes = 1000\n",
        "num_steps = 1000\n",
        "rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()[0]\n",
        "    for step in range(num_steps):\n",
        "        #print(state)\n",
        "        action = simple_policy(state)\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        ep_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    rewards.append(ep_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "nzdjBfiA-yab",
        "outputId": "26c75b81-87a8-47cc-ee14-6c06abf2b4e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.boxplot(rewards, vert=False)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5H1zlxK-wb4",
        "outputId": "4c0af2f7-1a36-407e-f2e7-6522ec1da75d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward: 41.993\n"
          ]
        }
      ],
      "source": [
        "print('mean reward:', np.mean(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2so9LpTSEb33"
      },
      "source": [
        "### Neural Network Policy\n",
        "\n",
        "Now we try a more sophisticated policy: let's use a neural network.\n",
        "\n",
        "The network will take **state as input**. The output node will contain the probability of the actions.\n",
        "\n",
        "Since there are two actions (left, right), we require one output node.  \n",
        "Node will output probability of right (so prob of left is implied).\n",
        "\n",
        "For simplicity, we will use one hidden layer.\n",
        "\n",
        "Number of nodes in hidden layer is a hyperparameter.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIb4dzbRrkzU"
      },
      "source": [
        "\n",
        "#### 1) **Define a neural network model for the policy.**\n",
        "\n",
        "**(POINTS: 1)**  \n",
        "It should have appropriate dimensions for the input and output layer. Print a summary of the model that shows the output shape for each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFXOXI_jCtgi",
        "outputId": "edc75812-102b-4a1b-bec2-9a14793fde07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PolicyNetwork(\n",
            "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the number of nodes in the hidden layer as a hyperparameter\n",
        "HIDDEN_LAYER_SIZE = 32 #32 gives better results than 64\n",
        "\n",
        "# Define the neural network model using PyTorch\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, observation_space_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(observation_space_size, HIDDEN_LAYER_SIZE) # Input layer to hidden layer\n",
        "        self.relu = nn.ReLU() # ReLU activation\n",
        "        self.fc2 = nn.Linear(HIDDEN_LAYER_SIZE, 1) # Hidden layer to output layer\n",
        "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for probability of action 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        action_prob = self.sigmoid(x)\n",
        "        return action_prob\n",
        "\n",
        "# Instantiate the model\n",
        "model = PolicyNetwork(env.observation_space.shape[0])\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHftbUIR8YMf"
      },
      "source": [
        "#### 2) Is **REINFORCE** a Monte Carlo method? Explain your answer.\n",
        "\n",
        "**(POINTS: 1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfkZsHku8Yh6"
      },
      "source": [
        "Yes, REINFORCE is a Monte Carlo method. It collects the rewards from the entire episode before calculating the discounted return. This return is then used to update the policy parameters using gradient ascent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsmQEX73IA73"
      },
      "source": [
        "\n",
        "\n",
        "#### 3) **Training and Evaluating the Policy Model.**\n",
        "\n",
        "Define functions to train and evaluate the model. Your work should include the steps below among others. Note that subtasks are numbered [1], [2], etc. and are worth one point each.\n",
        "\n",
        "- **(POINTS: 2)** Write a function `play_single_step` to evolve the system one step. It should also compute the gradient of the loss function. Test that it works properly and print the next state.  \n",
        "\n",
        "- **(POINTS: 2)** Write a function that runs multiple episodes, calling the `play_single_step` function. It should also compute and store the reward and gradient for each time step of each episode. Test that it works properly and print the rewards from running two episodes each with five time steps.\n",
        "- **(POINTS: 3)** Define and run a training loop using the REINFORCE algorithm that [1] runs multiple episodes, [2] computes discounted rewards, and [3] updates the parameters with gradient ascent. Run a sufficient number of episodes and time steps to see an average reward of at least 75 in the `evaluate` step that follows. Show evidence that the training loop is working, such as printing the total discounted rewards per episode.\n",
        "- **(POINTS: 3)** Now that the model is trained, you can use it as the policy and evaluate performance. Write code that [1] applies the model as the policy, [2] runs 1000 episodes and [3] computes the minimum reward, average reward, and maximum reward across the episodes. Discuss how the average reward from REINFORCE compares to the average reward from the simple policy from Cart Pole lab 1.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "h5F4i77ECzUo"
      },
      "outputs": [],
      "source": [
        "def play_single_step(state, model, env):\n",
        "    # Convert state to PyTorch tensor\n",
        "    state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
        "\n",
        "    # Get action probability from the model\n",
        "    action_prob = model(state_tensor) #calling model automatically runs the forward() function\n",
        "\n",
        "    # Sample an action from the probability distribution\n",
        "    # For a single output representing P(action=1) is right, P(action=0) is left = 1 - P(action=1)\n",
        "    action_distribution = torch.distributions.Categorical(torch.cat([1 - action_prob, action_prob], dim=1))\n",
        "    action = action_distribution.sample()\n",
        "\n",
        "    # Calculate the log probability of the chosen action\n",
        "    log_prob = action_distribution.log_prob(action)\n",
        "\n",
        "    # Take the chosen action in the environment\n",
        "    next_state, reward, terminated, truncated, info = env.step(action.item())\n",
        "    done = terminated or truncated\n",
        "\n",
        "    # For REINFORCE, the \"loss\" for a single step is the negative log probability of the action\n",
        "    # We will multiply this by the discounted reward later in the training loop\n",
        "    loss = -log_prob\n",
        "\n",
        "    return next_state, reward, done, loss, log_prob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "609fc795",
        "outputId": "231406f1-1649-4c4f-e651-cbbe19cc4b46"
      },
      "source": [
        "# Test the play_single_step function\n",
        "# Reset the environment to get an initial state\n",
        "state = env.reset(seed=42)[0]\n",
        "\n",
        "# Play a single step\n",
        "next_state, reward, done, loss, log_prob = play_single_step(state, model, env)\n",
        "\n",
        "# Print the next state\n",
        "print(\"Next State:\", next_state)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Done:\", done)\n",
        "print(\"Loss (negative log probability):\", loss)\n",
        "print(\"Log Probability:\", log_prob)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next State: [ 0.02727336 -0.20172954  0.03625453  0.32351476]\n",
            "Reward: 1.0\n",
            "Done: False\n",
            "Loss (negative log probability): tensor([0.6105], grad_fn=<NegBackward0>)\n",
            "Log Probability: tensor([-0.6105], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "115c93cf"
      },
      "source": [
        "#play_multiple_steps\n",
        "def play_multiple_episodes(model, env, num_episodes, num_steps):\n",
        "    all_rewards = []\n",
        "    all_losses = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        episode_losses = []\n",
        "        state = env.reset(seed=episode)[0] # Use episode number as seed for reproducibility\n",
        "        done = False\n",
        "        step_count = 0\n",
        "\n",
        "        while not done and step_count < num_steps:\n",
        "            next_state, reward, done, loss, log_prob = play_single_step(state, model, env)\n",
        "            episode_rewards.append(reward)\n",
        "            episode_losses.append(loss)\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "\n",
        "        all_rewards.append(episode_rewards)\n",
        "        all_losses.append(episode_losses)\n",
        "\n",
        "    return all_rewards, all_losses"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrATX1Czbfyg",
        "outputId": "96d88f68-130e-4d8e-f573-1532ed63b080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewards from test episodes:\n",
            "Episode 1: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Episode 2: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "\n",
            "Losses from test episodes:\n",
            "Episode 1: [tensor([0.7821], grad_fn=<NegBackward0>), tensor([0.6089], grad_fn=<NegBackward0>), tensor([0.6121], grad_fn=<NegBackward0>), tensor([0.7780], grad_fn=<NegBackward0>), tensor([0.7796], grad_fn=<NegBackward0>)]\n",
            "Episode 2: [tensor([0.7922], grad_fn=<NegBackward0>), tensor([0.6044], grad_fn=<NegBackward0>), tensor([0.6035], grad_fn=<NegBackward0>), tensor([0.6068], grad_fn=<NegBackward0>), tensor([0.7815], grad_fn=<NegBackward0>)]\n"
          ]
        }
      ],
      "source": [
        "# Test the play_multiple_episodes function\n",
        "num_test_episodes = 2\n",
        "num_test_steps = 5\n",
        "\n",
        "test_rewards, test_losses = play_multiple_episodes(model, env, num_test_episodes, num_test_steps)\n",
        "\n",
        "# Print the rewards from the test run\n",
        "print(\"Rewards from test episodes:\")\n",
        "for i, episode_rewards in enumerate(test_rewards):\n",
        "    print(f\"Episode {i+1}: {episode_rewards}\")\n",
        "\n",
        "# Also print the losses to see their structure\n",
        "print(\"\\nLosses from test episodes:\")\n",
        "for i, episode_losses in enumerate(test_losses):\n",
        "  print(f\"Episode {i+1}: {episode_losses}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3863be67"
      },
      "source": [
        "## TRAINING LOOP\n",
        "\n",
        "I ran the training loop two times. The first is using normalized rewards, anad the second without normalized rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7b6ce1b"
      },
      "source": [
        "# training loop using normalized rewards\n",
        "def discount_rewards(rewards, gamma=0.99):  #helper function to calculate discounted rewards\n",
        "    discounted_reward = 0\n",
        "    discounted_rewards = []\n",
        "    for reward in reversed(rewards):\n",
        "        discounted_reward = reward + gamma * discounted_reward\n",
        "        discounted_rewards.append(discounted_reward)\n",
        "    discounted_rewards.reverse()\n",
        "    return torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "def train_policy(model, env, optimizer, num_training_episodes, max_steps_per_episode, gamma=0.99, use_normalization=True):\n",
        "    for episode in range(num_training_episodes):\n",
        "        episode_rewards = []\n",
        "        episode_losses = [] # These are the negative log probabilities\n",
        "\n",
        "        state = env.reset(seed=episode)[0]\n",
        "        done = False\n",
        "        step_count = 0\n",
        "\n",
        "        while not done and step_count < max_steps_per_episode:\n",
        "            next_state, reward, done, loss, log_prob = play_single_step(state, model, env)\n",
        "            episode_rewards.append(reward)\n",
        "            episode_losses.append(loss)\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "\n",
        "        # Calculate discounted rewards for the episode\n",
        "        discounted_returns = discount_rewards(episode_rewards, gamma)\n",
        "\n",
        "        # Normalize discounted rewards for training stability\n",
        "        if use_normalization:\n",
        "            discounted_returns = (discounted_returns - discounted_returns.mean()) / (discounted_returns.std() + 1e-9)\n",
        "\n",
        "        # Calculate the policy loss for the episode\n",
        "        # The loss is the sum of (negative log probability * discounted return) for each step\n",
        "        policy_loss = torch.cat(episode_losses).squeeze() * discounted_returns\n",
        "        policy_loss = policy_loss.mean() # Take the mean to average the loss over steps\n",
        "\n",
        "        # Perform gradient ascent (minimize the negative policy loss)\n",
        "        optimizer.zero_grad() # Clear previous gradients\n",
        "        policy_loss.backward() # Compute gradients\n",
        "        optimizer.step() # Update model parameters\n",
        "\n",
        "        # Print progress\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode {episode+1}/{num_training_episodes}, Total steps: {step_count}, Total reward: {sum(episode_rewards)}, Policy Loss: {policy_loss.item():.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dbb8939"
      },
      "source": [
        "## Run training without normalization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb39113b",
        "outputId": "a165c415-d85d-463d-8d51-c6c9601a92d9"
      },
      "source": [
        "# Instantiate a new model for training without normalization\n",
        "model_no_norm = PolicyNetwork(env.observation_space.shape[0])\n",
        "\n",
        "# Instantiate a new optimizer for the new model\n",
        "optimizer_no_norm = optim.Adam(model_no_norm.parameters(), lr=0.01)\n",
        "\n",
        "# Set training parameters\n",
        "num_training_episodes = 1000\n",
        "max_steps_per_episode = 500\n",
        "\n",
        "# Train the model without normalization\n",
        "print(\"Starting training without normalization...\")\n",
        "train_policy(model_no_norm, env, optimizer_no_norm, num_training_episodes, max_steps_per_episode, use_normalization=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training without normalization...\n",
            "Episode 100/1000, Total steps: 21, Total reward: 21.0, Policy Loss: 6.6692\n",
            "Episode 200/1000, Total steps: 30, Total reward: 30.0, Policy Loss: 7.5851\n",
            "Episode 300/1000, Total steps: 82, Total reward: 82.0, Policy Loss: 16.4352\n",
            "Episode 400/1000, Total steps: 112, Total reward: 112.0, Policy Loss: 24.3651\n",
            "Episode 500/1000, Total steps: 38, Total reward: 38.0, Policy Loss: 7.1896\n",
            "Episode 600/1000, Total steps: 197, Total reward: 197.0, Policy Loss: 30.5856\n",
            "Episode 700/1000, Total steps: 273, Total reward: 273.0, Policy Loss: 32.8052\n",
            "Episode 800/1000, Total steps: 189, Total reward: 189.0, Policy Loss: 26.1525\n",
            "Episode 900/1000, Total steps: 121, Total reward: 121.0, Policy Loss: 19.0503\n",
            "Episode 1000/1000, Total steps: 238, Total reward: 238.0, Policy Loss: 28.1217\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63a4476c"
      },
      "source": [
        "### Run training with normalization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "570a7bac",
        "outputId": "12cc4c20-e6ce-44ff-d8a8-38a2a9641248"
      },
      "source": [
        "# Instantiate a new model for training with normalization\n",
        "model_with_norm = PolicyNetwork(env.observation_space.shape[0])\n",
        "\n",
        "# Instantiate a new optimizer for the new model\n",
        "optimizer_with_norm = optim.Adam(model_with_norm.parameters(), lr=0.01)\n",
        "\n",
        "# Set training parameters (same as before)\n",
        "num_training_episodes = 1000\n",
        "max_steps_per_episode = 500\n",
        "\n",
        "# Train the model with normalization\n",
        "print(\"Starting training with normalization...\")\n",
        "train_policy(model_with_norm, env, optimizer_with_norm, num_training_episodes, max_steps_per_episode, use_normalization=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with normalization...\n",
            "Episode 100/1000, Total steps: 37, Total reward: 37.0, Policy Loss: -0.0014\n",
            "Episode 200/1000, Total steps: 413, Total reward: 413.0, Policy Loss: -0.0050\n",
            "Episode 300/1000, Total steps: 500, Total reward: 500.0, Policy Loss: -0.0047\n",
            "Episode 400/1000, Total steps: 500, Total reward: 500.0, Policy Loss: 0.0145\n",
            "Episode 500/1000, Total steps: 410, Total reward: 410.0, Policy Loss: -0.0399\n",
            "Episode 600/1000, Total steps: 500, Total reward: 500.0, Policy Loss: 0.0025\n",
            "Episode 700/1000, Total steps: 500, Total reward: 500.0, Policy Loss: 0.0104\n",
            "Episode 800/1000, Total steps: 500, Total reward: 500.0, Policy Loss: 0.0259\n",
            "Episode 900/1000, Total steps: 500, Total reward: 500.0, Policy Loss: 0.0159\n",
            "Episode 1000/1000, Total steps: 500, Total reward: 500.0, Policy Loss: -0.0059\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POLICY EVALUATION"
      ],
      "metadata": {
        "id": "7kuK3LzOZP8S"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ca543bb"
      },
      "source": [
        "# evaluate policy performance\n",
        "\n",
        "def evaluate_policy(model, env, num_episodes):\n",
        "    evaluation_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset(seed=episode + 1000)[0] # Use a different seed range for evaluation\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            # Get action probability from the trained model\n",
        "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
        "            action_prob = model(state_tensor)\n",
        "\n",
        "            # Choose the action with the highest probability (deterministic policy for evaluation)\n",
        "\n",
        "            action = 1 if action_prob.item() > 0.5 else 0\n",
        "\n",
        "\n",
        "            # Take the chosen action in the environment\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        evaluation_rewards.append(episode_reward)\n",
        "\n",
        "    return evaluation_rewards"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5305267"
      },
      "source": [
        "### Evaluate with normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "add680ce",
        "outputId": "24f0954a-4139-4ae5-c4e8-fac55fbca8d0"
      },
      "source": [
        "# Evaluate the performance of the model trained with normalization\n",
        "evaluation_rewards_with_norm = evaluate_policy(model_with_norm, env, num_evaluation_episodes)\n",
        "\n",
        "# Calculate and print the results\n",
        "min_reward_with_norm = np.min(evaluation_rewards_with_norm)\n",
        "max_reward_with_norm = np.max(evaluation_rewards_with_norm)\n",
        "average_reward_with_norm = np.mean(evaluation_rewards_with_norm)\n",
        "\n",
        "print(f\"Evaluation over {num_evaluation_episodes} episodes (with normalization):\")\n",
        "print(f\"Minimum Reward: {min_reward_with_norm}\")\n",
        "print(f\"Maximum Reward: {max_reward_with_norm}\")\n",
        "print(f\"Average Reward: {average_reward_with_norm}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation over 1000 episodes (with normalization):\n",
            "Minimum Reward: 500.0\n",
            "Maximum Reward: 500.0\n",
            "Average Reward: 500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u54ZcJbVfetI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a87142b"
      },
      "source": [
        "### Evaluate without normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3021f2a",
        "outputId": "2caeab67-7563-4875-b6fa-a81156d3fff3"
      },
      "source": [
        "# Evaluate the performance of the model trained without normalization\n",
        "num_evaluation_episodes = 1000 # Use the same number of episodes as the other evaluation\n",
        "evaluation_rewards_no_norm = evaluate_policy(model_no_norm, env, num_evaluation_episodes)\n",
        "\n",
        "# Calculate and print the results\n",
        "min_reward_no_norm = np.min(evaluation_rewards_no_norm)\n",
        "max_reward_no_norm = np.max(evaluation_rewards_no_norm)\n",
        "average_reward_no_norm = np.mean(evaluation_rewards_no_norm)\n",
        "\n",
        "print(f\"Evaluation over {num_evaluation_episodes} episodes (without normalization):\")\n",
        "print(f\"Minimum Reward: {min_reward_no_norm}\")\n",
        "print(f\"Maximum Reward: {max_reward_no_norm}\")\n",
        "print(f\"Average Reward: {average_reward_no_norm}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation over 1000 episodes (without normalization):\n",
            "Minimum Reward: 223.0\n",
            "Maximum Reward: 307.0\n",
            "Average Reward: 263.256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d03b11c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "I used Gemini AI in Colab to assist in code generation. This was a helpful aid in navigating the nuances of PyTorch for ensuring tensors were in the expected format for the code to run successfully. This enabled me to focus on experimenting with different RL techniques, such as adding a baseline, rather than struggling with PyTorch. This enhanced my learning overall.\n",
        "\n",
        "### Comparison of Policy Vs Simple Policy\n",
        "The simple rule-based policy for the CartPole, which merely pushed the cart in the direction the pole was leaning, achieved an average reward of approximately 42 over 1000 episodes. This indicated a basic ability to keep the pole upright for a short duration. In stark contrast, the policy gradient approach using a neural network, particularly when trained with normalized discounted returns, achieved a perfect average reward of 500 over 1000 evaluation episodes. This demonstrates that the policy gradient method was able to learn a far more sophisticated and effective strategy for balancing the pole, consistently reaching the maximum episode length by adjusting the neural network's parameters based on the outcomes of its actions over many training episodes.\n",
        "\n",
        "Looking at the printed progress messages, the \"Total reward\" for episodes significantly increases relatively early in training. By Episode 300, the total reward reached 500.0. This suggests that with the aid of normalization, the policy learned to consistently solve the environment (balance for 500 steps) within the first few hundred training episodes.\n",
        "\n",
        "The experiments utilize a simple feedforward neural network as the policy model. This network takes the 4-dimensional state of the CartPole environment as input and has a single hidden layer with a configurable number of nodes (set as HIDDEN_LAYER_SIZE). I chose 32 nodes as the hidden layer, but also ran with 64 nodes but achieved poorer resuls. I used ReLu activation for thehidden layer . The output layer consists of a single neuron with a Sigmoid activation, which is interpreted as the probability of taking the \"move right\" action. The REINFORCE policy gradient algorithm is used for training. I used the Adam optimizer with a learning rate of 0.01. The training process involves running episodes, calculating discounted rewards for each step, and using these returns to update the policy network via gradient ascent (implemented by minimizing the negative policy loss). The training was performed for 1000 episodes with a maximum of 500 steps per episode. Evaluation was conducted using a deterministic policy derived from the trained network, running for 1000 episodes to assess performance.\n",
        "\n",
        "*   **Experiment 1: Training without Normalization**\n",
        "    *   Training was performed for 1000 episodes.\n",
        "    *   Evaluation over 1000 episodes resulted in an average reward of **263.256**.\n",
        "    *   The minimum reward observed was **223.0**, and the maximum was **307.0**.\n",
        "    *   The policy learned to balance the pole significantly better than the simple policy (average reward ~42), but did not consistently reach the maximum possible reward of 500.\n",
        "\n",
        "*   **Experiment 2: Training with Normalization**\n",
        "    *   Training was also performed for 1000 episodes, with the addition of normalizing discounted returns.\n",
        "    *   Evaluation over 1000 episodes resulted in an average reward of **500.0**.\n",
        "    *   The minimum reward observed was **500.0**, and the maximum was **500.0**.\n",
        "    *   The policy achieved the maximum possible reward in every single evaluation episode.\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "*   Normalizing the discounted returns during training dramatically improved the policy network's performance on the CartPole environment in this case.\n",
        "*   Training with normalization led to a policy that consistently solved the environment, achieving the maximum reward in all evaluated episodes, while training without it resulted in a competent but not optimal policy.\n",
        "*   This demonstrates that normalization can be a crucial technique for stabilizing and improving the performance of policy gradient methods like REINFORCE."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}