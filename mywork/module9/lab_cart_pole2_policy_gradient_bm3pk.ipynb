{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GBod1nv65dN"
      },
      "source": [
        "## Lab: Cart Pole Demo 2 using OpenAI gym\n",
        "## Policy Gradient\n",
        "\n",
        "### University of Virginia\n",
        "### Reinforcement Learning\n",
        "#### Last updated: March 4, 2024\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J73-oPOTqC5b"
      },
      "source": [
        "#### Instructions:  \n",
        "\n",
        "Carefully read the notes below and run the provided code. Answer each question clearly and show all results.\n",
        "\n",
        "#### TOTAL POINTS: 12\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6__XiTWDIiT"
      },
      "source": [
        "### First a Refresh\n",
        "\n",
        "Let's briefly review the content from Demo 1: Basics and Simple Policy\n",
        "\n",
        "We revisit the CartPole problem.\n",
        "\n",
        "We will work with the fork [gymnasium](https://gymnasium.farama.org/) which maintains OpenAI gym.  \n",
        "\n",
        "The *simple policy* didn't perform very well: the average reward was about 42.\n",
        "\n",
        "We want to see if we can do better using a Policy Gradient algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA_dd0h6Eoap"
      },
      "source": [
        "### Setup and First Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wM5RiqAeqHJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22845e3f-1800-4211-c4e2-c12c8f06245c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M4HpQPsAqH4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1d4747-c470-44d5-dfd2-e4abc8e83897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting renderlab\n",
            "  Downloading renderlab-0.1.20230421184216-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.0.3)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from renderlab) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->renderlab) (0.0.4)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->renderlab) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy->renderlab) (11.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2025.10.5)\n",
            "Downloading renderlab-0.1.20230421184216-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: renderlab\n",
            "Successfully installed renderlab-0.1.20230421184216\n"
          ]
        }
      ],
      "source": [
        "! pip install renderlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ed_DyvHy6pbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28160ad-7f44-41e9-d517-2d28797ee73e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import renderlab as rl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA4JgtuL5jC9"
      },
      "source": [
        "Load the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UcGLRi6F6vcQ"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
        "state = env.reset(seed=314)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtQFJsxj7aBj"
      },
      "source": [
        "Given the state, we take an action. The next state comes from the environment, which is encoded in `gym`.\n",
        "\n",
        "Components:   \n",
        "[0]: cart horizontal position (0.0 = center)  \n",
        "[1]: velocity (positive means right)  \n",
        "[2]: angle of the pole (0.0 = vertical)  \n",
        "[3]: pole's angular velocity (positive means clockwise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M30S6Bf_6yNv",
        "outputId": "4c42e4dd-63fe-46ff-b94b-46d3be397c29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.04225422, 0.02126478, 0.02520455, 0.00700802], dtype=float32), {})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A5peNKiBXsh",
        "outputId": "e75e9319-88ec-4a9f-d971-1e304d545d37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# state space number of components\n",
        "env.observation_space.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GkZmsIN7ky6"
      },
      "source": [
        "The action space consists of two options:\n",
        "\n",
        "[0]: move cart left   \n",
        "[1]: move cart right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySQ8EpsV7Ngr",
        "outputId": "d761a633-6a13-419d-a1e7-22ef073c2852"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(2)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_68lpUuVCc-_"
      },
      "source": [
        "Let's take an action, draw a sample and look at the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKFEIHjL7n7h",
        "outputId": "32e35c95-895a-460e-e890-c10bcc2c0a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state [ 0.04267951  0.21601637  0.02534471 -0.27761722]\n",
            "reward 1.0\n",
            "done False\n",
            "info {}\n"
          ]
        }
      ],
      "source": [
        "# move right\n",
        "action = 1\n",
        "\n",
        "# take a step and get next state, reward from environment\n",
        "state, reward, terminated, truncated, info = env.step(action)\n",
        "done = terminated or truncated\n",
        "\n",
        "print('state', state)\n",
        "print('reward', reward)\n",
        "print('done', done)\n",
        "print('info', info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqxI0vej88Kh"
      },
      "source": [
        "**Reward and Episode**  \n",
        "\n",
        "For each time step that the cart keeps the pole balanced, it earns reward 1.\n",
        "\n",
        "If the pole tilts too much or if the cart moves off screen, `reward=0` and `done=True` (the episode will end).\n",
        "\n",
        "When the episode ends, a new episode may begin. The process learns cumulatively from each episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv5gKVde9pvQ"
      },
      "source": [
        "**Simple policy**:  \n",
        "\n",
        "When the pole leans left (negative angle), move left. When the pole leans right (positive angle), move right.\n",
        "\n",
        "Run many episodes and visualize their reward distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dREsiRKf9CbR"
      },
      "outputs": [],
      "source": [
        "def simple_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "num_episodes = 1000\n",
        "num_steps = 1000\n",
        "rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()[0]\n",
        "    for step in range(num_steps):\n",
        "        #print(state)\n",
        "        action = simple_policy(state)\n",
        "        state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        ep_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    rewards.append(ep_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "nzdjBfiA-yab",
        "outputId": "9b0d21c2-6c4a-462d-9265-965a0b14224b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAGdCAYAAAC2OMGiAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGCZJREFUeJzt3X2QlWX9+PHPkd1FVnaBMHkQEEIUTTBFJFLLp0QzZ9F0nBIHx2y+JuZTg1DfKXKm0hE1swdNbdLR1MlUSMuUUkkbRJ7MhwSBCDQQxxJYHmQX9v79Uey3FfypuH7OAV6vmR3gnOuc65q9uHffe597d0tFURQBAPAh263cCwAAdg2iAwBIIToAgBSiAwBIIToAgBSiAwBIIToAgBSiAwBIUZU9YUtLSyxfvjzq6uqiVCplTw8AbIeiKKKxsTF69+4du+22fecs0qNj+fLl0bdv3+xpAYB28Morr0SfPn2267Hp0VFXVxcR/150fX199vQ7nebm5nj00UfjhBNOiOrq6nIvh/+wL5XL3lQm+1K5tuzNyJEjY8CAAa2fx7dHenRseUmlvr5edLSD5ubmqK2tjfr6egdqBbEvlcveVCb7Urm27M2W2Pggl0a4kBQASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASCE6AIAUogMASFFV7gXAh2nhwoXR2NiYPu+mTZti8eLFMW/evKiqcphtS11dXQwaNKjcywAS+WjITmvhwoWx3377lWXunp1L8T/DaqJhUlO8trYoyxp2BC+//LLwgF2I6GCnteUMx5133hkHHHBA6tw1/3wpDvrzBXH6//48mrrnzr0jeOmll2LMmDFlOQsFlI/oYKd3wAEHxKGHHpo6Z/Oylog/R+y///5R3S93boBK5UJSACCF6AAAUogOACCF6AAAUogOACCF6AAAUogOACCF6AAAUogOACCF6AAAUuw00bF+/fqYO3durF+/vtxLAaAC+LxQeXaa6Jg/f34MGzYs5s+fX+6lAFABfF6oPDtNdAAAlU10AAApRAcAkEJ0AAApRAcAkEJ0AAApRAcAkEJ0ALDL2rx5czzxxBNx9913xxNPPBGbN2/e5rgNGzbEhRdeGKNGjYoLL7wwNmzYsM1xTU1Ncf3118fXvva1uP7666Opqekd5167dm2ceuqpMXTo0Dj11FNj7dq12xy3evXqOPLII6Nfv35x5JFHxurVqz/QGsuqeJ+mT59efP7zny969epVRETxwAMPvK/Hr169uoiIYvXq1e936v+vOXPmFBFRzJkzp12ft9I1NTUVU6ZMKZqamsq9lIpTzv8TTUtnFcWk+n//yVbKujeOmYr0YezLu/0/u++++4r+/fsXEdH61r9//+K+++5rM66hoaHNmC1vDQ0NbcaNHz++qKqqajOmqqqqGD9+/FZzDx8+fJvPOXz48DbjBg4cuM1xAwcO3K41bo8te/PGG2984M/f7/tMx7p16+Lggw+On/zkJ9uROABQfvfff3+cfvrpMWTIkJgxY0Y0NjbGjBkzYsiQIXH66afH/fffHxERo0ePjqlTp0ZNTU1MnDgxFi1aFBMnToyampqYOnVqjB49OiIiLr/88pg8eXJ07949brnlllixYkXccsst0b1795g8eXJcfvnlrXMffvjhMWvWrCiVSnH22WfHX/7ylzj77LOjVCrFrFmz4vDDD4+IiH333TcWL14cEREnnnhizJgxI0488cSIiFi8eHHsu+++72uNFeGD1E8401F2vmp7Z850VC5nOni7zDMdmzZtKvr371+ccsopxebNm9vct3nz5uKUU04pBgwYUDQ2NhYRUdTU1BQbN25sM27jxo1FTU1NERHFqlWriqqqqqJHjx5Fc3Nzm3HNzc1Fjx49iqqqqmLjxo2tz1kqlYoNGza0Gbthw4aiVCoVEVG8+uqrrWcr1q1b12bcunXrWu9bsWLFe1rj+vXrt+t9WBTte6aj6sOOmo0bN8bGjRtb/71mzZqIiGhubo7m5uZ2m6exsTEiIl544YXYtGlTuz1vpWtubo7FixfHM888E9XV1eVeTkXZ8vsW1q5d267/196LTZs2RfV//ozkuXcEW167Lsfx6pipTB/Gvmz5GNDY2NjmY8D06dPj73//e9xxxx2xefPmra7jGD9+fHz605+OL33pSxERcckll0SpVGrzHKVSKS666KK45ppr4qSTTopNmzbFFVdcEUVRbPXxZtKkSXHBBRfEj370o5g+fXpERJx11lnRoUOHNmM7dOgQX/ziF+Ouu+6KIUOGRETEqFGjorq6us246urq+OxnPxvTpk2LT3ziE+9pjZdddlnccMMN2/V+3PKc7fFx9EOPjiuvvDKuuOKKrW5/9NFHo7a2tt3m2bKRY8eObbfnZOcwZcqUePPNN1Pn7LL+73F0RMycOTNWP78yde4dgeOVTFOnTo1Vq1a1/vtPf/pTRES8+uqr8c9//nOr8VsuwHzuueciImLAgAHxu9/9bqtxAwYMiIiIhQsXRkREx44dtzlu9913j4iIxx57LF588cWIiBg2bNg2xw4bNizuuuuu1i/QjznmmG2OO/roo2PatGnxr3/96z2t8emnn97m/e/H448//oEeH5EQHd/4xjfisssua/33mjVrom/fvnHCCSdEfX19u83TtWvX+MEPfhC33357DB48uN2et9I1NzfHzJkzY8SIEb5qe5v58+fH2LFjY/To0TFy5MjUuTe9MidiQcSIESOiqu+w1Ll3BN26dSvb8eqYqUwfxr5s+RjQ0NAQn/rUp1pv32OPPeK6666LPn36xIgRI7Z63NNPPx0REUOHDo2lS5fGkiVL4stf/vJW4775zW9GRMSgQYPijTfeiI0bN8bnPve5rcbdeuutERFx7LHHRocOHWLp0qUxZ86cGDdu3FZjzznnnIiIqK+vjzfffDMef/zxNp9Dt9hyXeVHPvKRWLly5buu8ZOf/OQ21/ZeNDc3x7Rp0+KYY47Zrse3sd0vzBSu6agEXp9+Z67pqFyu6eDtXNOxa1zT4ed0ALBL6dChQ1x77bXx0EMPxejRo9t898ro0aPjoYceimuuuSY6d+4cDQ0N0dTUFHV1dTFhwoR4+eWXY8KECVFXVxdNTU3R0NAQXbp0iUsvvTRWrlwZffr0iZtvvjmWL18eN998c/Tp0ydWrlwZl156adTU1ETnzp1j+PDhURRF1NbWxpgxY2Lu3LkxZsyYqK2tjaIoYvjw4bH33nvHwIEDI+LfZ2ZGjRoVTz75ZIwaNSr22GOPiIgYOHBg9OzZ8z2tsVOnTuV8l/+f91spjY2Nxbx584p58+YVEVFcd911xbx584qlS5e+p8c709G+fNX2zpzpqFzOdPB2lfJzOgYMGODndLxNWb97Zfbs2W1e19nyWtPYsWPjtttue79PBwBlcdppp0VDQ0M8+eSTsWLFiujVq1ccddRR0aFDhzbjpkyZEhs2bIjx48fHwoULY9CgQTF58uStzh5cffXV8d3vfjd++tOfxuLFi2PgwIFxwQUXRE1NzVZzP/PMM7F27do4++yzW8fecccd0blz5zbjFi1aFKtXr46TTz45li1bFv369Yvf/va30aVLl+1aY7m97+g4+uijoyiKD2MtAJCqQ4cOcfTRR7/ruE6dOsWPf/zjdx1XU1MTl1xyyXuau3PnzvHAAw+867guXbrEU0899a7j3usay8k1HQBACtEBAKQQHQBACtEBAKQQHQBACtEBAKQQHQBACtEBAKTYaaJj8ODBMWfOnF3qN8wC8M58Xqg8H/qvts9SW1sbhx56aLmXAUCF8Hmh8uw0ZzoAgMomOgCAFKIDAEghOgCAFKIDAEghOgCAFKIDAEghOgCAFKIDAEghOgCAFDvNj0GHt1u/fn1ERMydOzd97pp/LoiDImLBggXR9Ia2f7uXXnqp3EsAykB0sNOaP39+RER85StfSZ+7Z+dS/M+wmvjZtWfHa2uL9Pl3FHV1deVeApBIdLDTGj16dET8+zdN1tbWps69adOmeOqpp2Lq1UdGVZXDbFvq6upi0KBB5V4GkMhHQ3Zae+65Z5x33nllmbu5uTlWrFgRhxxySFRXV5dlDQCVxovNAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAEAK0QEApBAdAECKquwJi6KIiIg1a9ZkT71Tam5ujvXr18eaNWuiurq63MvhP+xL5bI3lcm+VK4te9PY2BgR//d5fHukR8eWRfft2zd7agDgA2psbIwuXbps12NLxQdJlu3Q0tISy5cvj7q6uiiVSplT75TWrFkTffv2jVdeeSXq6+vLvRz+w75ULntTmexL5dqyN8uWLYtSqRS9e/eO3Xbbvqsz0s907LbbbtGnT5/saXd69fX1DtQKZF8ql72pTPalcnXp0uUD740LSQGAFKIDAEghOnZwHTt2jEmTJkXHjh3LvRT+i32pXPamMtmXytWee5N+ISkAsGtypgMASCE6AIAUogMASCE6AIAUomMHcOONN8bQoUNbf2jOyJEj4+GHH269/6233opx48ZF9+7do3PnzvGFL3whVq5cWcYV75quuuqqKJVKcckll7TeZm/K4zvf+U6USqU2b4MHD269376Uzz/+8Y8YM2ZMdO/ePTp16hRDhgyJ2bNnt95fFEV8+9vfjl69ekWnTp3i+OOPj4ULF5ZxxbuG/v37b3XMlEqlGDduXES03zEjOnYAffr0iauuuirmzJkTs2fPjmOPPTYaGhrixRdfjIiISy+9NB588MG49957Y/r06bF8+fI47bTTyrzqXcusWbPiZz/7WQwdOrTN7famfD7+8Y/HihUrWt+eeuqp1vvsS3m8+eabccQRR0R1dXU8/PDD8de//jWuvfba6NatW+uYq6++Om644Ya46aabYubMmbHHHnvEqFGj4q233irjynd+s2bNanO8TJs2LSIizjjjjIhox2OmYIfUrVu34tZbby1WrVpVVFdXF/fee2/rfS+99FIREcWMGTPKuMJdR2NjYzFo0KBi2rRpxWc+85ni4osvLoqisDdlNGnSpOLggw/e5n32pXwmTJhQHHnkke94f0tLS9GzZ89i8uTJrbetWrWq6NixY3H33XdnLJH/uPjii4uBAwcWLS0t7XrMONOxg9m8eXPcc889sW7duhg5cmTMmTMnmpub4/jjj28dM3jw4OjXr1/MmDGjjCvddYwbNy5OPvnkNnsQEfamzBYuXBi9e/eOj33sY3HWWWfFsmXLIsK+lNNvfvObOOyww+KMM86IvfbaKw455JC45ZZbWu9fsmRJvPbaa232pkuXLjFixAh7k6ipqSnuvPPOOPfcc6NUKrXrMSM6dhDPP/98dO7cOTp27Bjnn39+PPDAA3HggQfGa6+9FjU1NdG1a9c243v06BGvvfZaeRa7C7nnnnti7ty5ceWVV251n70pnxEjRsRtt90Wv//97+PGG2+MJUuWxFFHHRWNjY32pYz+9re/xY033hiDBg2KRx55JL761a/GRRddFLfffntEROv7v0ePHm0eZ29yTZkyJVatWhXnnHNORLTvx7L03zLL9tl///3j2WefjdWrV8evf/3rGDt2bEyfPr3cy9qlvfLKK3HxxRfHtGnTYvfddy/3cvgvJ510Uuvfhw4dGiNGjIh99tknfvWrX0WnTp3KuLJdW0tLSxx22GHx/e9/PyIiDjnkkHjhhRfipptuirFjx5Z5dWzx85//PE466aTo3bt3uz+3Mx07iJqamth3331j2LBhceWVV8bBBx8cP/zhD6Nnz57R1NQUq1atajN+5cqV0bNnz/IsdhcxZ86ceP311+PQQw+NqqqqqKqqiunTp8cNN9wQVVVV0aNHD3tTIbp27Rr77bdfLFq0yDFTRr169YoDDzywzW0HHHBA60tfW97/b/+uCHuTZ+nSpfGHP/whzjvvvNbb2vOYER07qJaWlti4cWMMGzYsqqur449//GPrfQsWLIhly5bFyJEjy7jCnd9xxx0Xzz//fDz77LOtb4cddlicddZZrX+3N5Vh7dq1sXjx4ujVq5djpoyOOOKIWLBgQZvbXn755dhnn30iImLAgAHRs2fPNnuzZs2amDlzpr1J8otf/CL22muvOPnkk1tva9djpr2veKX9TZw4sZg+fXqxZMmS4rnnnismTpxYlEql4tFHHy2KoijOP//8ol+/fsVjjz1WzJ49uxg5cmQxcuTIMq961/Tf371SFPamXL7+9a8XTzzxRLFkyZLiz3/+c3H88ccXe+65Z/H6668XRWFfyuWZZ54pqqqqiu9973vFwoULi1/+8pdFbW1tceedd7aOueqqq4quXbsWU6dOLZ577rmioaGhGDBgQLFhw4YyrnzXsHnz5qJfv37FhAkTtrqvvY4Z0bEDOPfcc4t99tmnqKmpKT760Y8Wxx13XGtwFEVRbNiwobjggguKbt26FbW1tcWpp55arFixoowr3nW9PTrsTXmceeaZRa9evYqamppi7733Ls4888xi0aJFrffbl/J58MEHi4MOOqjo2LFjMXjw4OLmm29uc39LS0vxrW99q+jRo0fRsWPH4rjjjisWLFhQptXuWh555JEiIrb5/m6vY8avtgcAUrimAwBIIToAgBSiAwBIIToAgBSiAwBIIToAgBSiAwBIIToAgBSiAwBIIToAgBSiAwBIIToAgBT/D7vBq7ms5XAvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.boxplot(rewards, vert=False)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5H1zlxK-wb4",
        "outputId": "3b82a991-b700-475b-9a53-568c2f16f470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward: 41.993\n"
          ]
        }
      ],
      "source": [
        "print('mean reward:', np.mean(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2so9LpTSEb33"
      },
      "source": [
        "### Neural Network Policy\n",
        "\n",
        "Now we try a more sophisticated policy: let's use a neural network.\n",
        "\n",
        "The network will take **state as input**. The output node will contain the probability of the actions.\n",
        "\n",
        "Since there are two actions (left, right), we require one output node.  \n",
        "Node will output probability of right (so prob of left is implied).\n",
        "\n",
        "For simplicity, we will use one hidden layer.\n",
        "\n",
        "Number of nodes in hidden layer is a hyperparameter.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIb4dzbRrkzU"
      },
      "source": [
        "\n",
        "#### 1) **Define a neural network model for the policy.**\n",
        "\n",
        "**(POINTS: 1)**  \n",
        "It should have appropriate dimensions for the input and output layer. Print a summary of the model that shows the output shape for each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFXOXI_jCtgi",
        "outputId": "7bc9f649-8443-4999-d840-72d19f01a480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PolicyNetwork(\n",
            "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the number of nodes in the hidden layer as a hyperparameter\n",
        "HIDDEN_LAYER_SIZE = 32\n",
        "\n",
        "# Define the neural network model using PyTorch\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, observation_space_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(observation_space_size, HIDDEN_LAYER_SIZE) # Input layer to hidden layer\n",
        "        self.relu = nn.ReLU() # ReLU activation\n",
        "        self.fc2 = nn.Linear(HIDDEN_LAYER_SIZE, 1) # Hidden layer to output layer\n",
        "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for probability of action 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        action_prob = self.sigmoid(x)\n",
        "        return action_prob\n",
        "\n",
        "# Instantiate the model\n",
        "model = PolicyNetwork(env.observation_space.shape[0])\n",
        "\n",
        "# Print the model\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHftbUIR8YMf"
      },
      "source": [
        "#### 2) Is **REINFORCE** a Monte Carlo method? Explain your answer.\n",
        "\n",
        "**(POINTS: 1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfkZsHku8Yh6"
      },
      "source": [
        "Yes, REINFORCE is a Monte Carlo method. It collects the rewards from the entire episode before calculating the discounted return. This return is then used to update the policy parameters using gradient ascent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsmQEX73IA73"
      },
      "source": [
        "\n",
        "\n",
        "#### 3) **Training and Evaluating the Policy Model.**\n",
        "\n",
        "Define functions to train and evaluate the model. Your work should include the steps below among others. Note that subtasks are numbered [1], [2], etc. and are worth one point each.\n",
        "\n",
        "- **(POINTS: 2)** Write a function `play_single_step` to evolve the system one step. It should also compute the gradient of the loss function. Test that it works properly and print the next state.  \n",
        "\n",
        "- **(POINTS: 2)** Write a function that runs multiple episodes, calling the `play_single_step` function. It should also compute and store the reward and gradient for each time step of each episode. Test that it works properly and print the rewards from running two episodes each with five time steps.\n",
        "- **(POINTS: 3)** Define and run a training loop using the REINFORCE algorithm that [1] runs multiple episodes, [2] computes discounted rewards, and [3] updates the parameters with gradient ascent. Run a sufficient number of episodes and time steps to see an average reward of at least 75 in the `evaluate` step that follows. Show evidence that the training loop is working, such as printing the total discounted rewards per episode.\n",
        "- **(POINTS: 3)** Now that the model is trained, you can use it as the policy and evaluate performance. Write code that [1] applies the model as the policy, [2] runs 1000 episodes and [3] computes the minimum reward, average reward, and maximum reward across the episodes. Discuss how the average reward from REINFORCE compares to the average reward from the simple policy from Cart Pole lab 1.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5F4i77ECzUo"
      },
      "outputs": [],
      "source": [
        "# play_single_step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrATX1Czbfyg"
      },
      "outputs": [],
      "source": [
        "# play_multiple_steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_WfJFsXbfyg"
      },
      "outputs": [],
      "source": [
        "# training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48IYDxC8bfyg"
      },
      "outputs": [],
      "source": [
        "# evaluate policy performance\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}