{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c",
      "metadata": {
        "id": "d2ba6549-8b15-4f95-9615-549dd5fa2f7c"
      },
      "source": [
        "### Lab: Value Iteration in a Grid World\n",
        "\n",
        "### University of Virginia\n",
        "### Reinforcement Learning\n",
        "#### Last updated: May 26, 2025\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45",
      "metadata": {
        "id": "2afeab41-bd5f-40e8-ad2f-e8999f13ed45"
      },
      "source": [
        "#### Instructions:\n",
        "\n",
        "Implement value iteration for a $4 \\times 3$ gridworld environment. This will measure the value of each state. A robot in this world can make discrete moves: one step up, down, left or right. These actions are deterministic, meaning that the action selected will be taken with probability 1. There is a terminal state with reward +1 in the bottom right corner. All other states have reward 0. The discount factor is 0.9. Use tolerance $\\theta=0.01$. Show all code and results.\n",
        "\n",
        "**Note**: Do not use libraries from `networkx`, `gym`, `gymnasium` when solving this problem.\n",
        "\n",
        "#### Total Points: 12"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5956322-1af2-4a18-b65c-d98dc08454c8",
      "metadata": {
        "id": "a5956322-1af2-4a18-b65c-d98dc08454c8"
      },
      "source": [
        "---\n",
        "\n",
        "#### 1) **(POINTS: 2)** As part of your solution, create a GridWorld class with these attributes:\n",
        "\n",
        "- `nrows` : number of rows in the grid\n",
        "- `ncols` : number of columns in the grid\n",
        "\n",
        "and these methods:\n",
        "\n",
        "- `value_iteration()` with behavior described in [2] below\n",
        "- `get_reward()` : given the agent row and column, return the reward\n",
        "\n",
        "The class may include additional attributes and methods as well.\n",
        "\n",
        "Create an instance using the class, and call `nrows`, `ncols`, and `get_reward()` to verify correctness.\n",
        "\n",
        "You will not be graded on the implementation of `value_iteration()` for this problem.\n",
        "\n",
        "#### 2) **(POINTS: 8)** Here, you will be graded on the implementation of `value_iteration()`.\n",
        "Call `value_iteration()` to calculate and return the value function array. For each sweep over the states, have the function print out the intermediate array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd213107-d461-431c-b1e1-d1d3c099976d",
      "metadata": {
        "id": "bd213107-d461-431c-b1e1-d1d3c099976d"
      },
      "source": [
        "#### Enter all code here (you may also use multiple cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338948a3-db77-4458-a6ed-a3f5e4aa3c72",
      "metadata": {
        "id": "338948a3-db77-4458-a6ed-a3f5e4aa3c72"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "858c5f68-7b54-459e-95f3-59697cfa6d23",
      "metadata": {
        "id": "858c5f68-7b54-459e-95f3-59697cfa6d23"
      },
      "source": [
        "#### 1) Create and test the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "a21a43af-2be7-49d3-aec0-1934160bb61c",
      "metadata": {
        "id": "a21a43af-2be7-49d3-aec0-1934160bb61c"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, nrows = 4, ncols = 3):\n",
        "        self.nrows = nrows\n",
        "        self.ncols = ncols\n",
        "        self.terminal = (nrows - 1, ncols - 1)\n",
        "\n",
        "    def get_reward(self, row, col):\n",
        "        if row == self.terminal[0] and col == self.terminal[1]:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def _print_values(self, V):\n",
        "        for r in range(self.nrows):\n",
        "            row = [V[(r, c)] for c in range(self.ncols)]\n",
        "            print(\" \".join(f\"{v:6.3f}\" for v in row))\n",
        "        print()\n",
        "\n",
        "\n",
        "    #helper function to check whether robot is inside the grid\n",
        "    def in_bounds(self, row, col):\n",
        "        return 0 <= row < self.nrows and 0 <= col < self.ncols\n",
        "\n",
        "\n",
        "    def value_iteration(self, theta = 0.01, gamma = 0.9):\n",
        "         #create empty dictionary to story values\n",
        "         V = {}\n",
        "         #set all the grid values to zero\n",
        "         for r in range(self.nrows):\n",
        "             for c in range(self.ncols):\n",
        "                 V[(r, c)] = 0.0\n",
        "         #set the value of the terminal state\n",
        "         V[self.terminal] = self.get_reward(r, c)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "         actions = [(-1, 0), # move up - subtract a row\n",
        "                    ( 1, 0), # move down - add a row\n",
        "                    ( 0, -1),# move left - subtract a col\n",
        "                    ( 0, 1)] # move right - add a column\n",
        "\n",
        "         while True:\n",
        "\n",
        "              delta = 0.0\n",
        "              newV = V.copy()\n",
        "\n",
        "              for r in range(self.nrows):\n",
        "                  for c in range(self.ncols):\n",
        "                     s = (r, c)\n",
        "                     if s == self.terminal:\n",
        "                        newV[s] = self.get_reward(r, c)\n",
        "                        continue\n",
        "\n",
        "                     best = float('-inf')\n",
        "                     for dr, dc in actions:\n",
        "                        new_r, new_c = r + dr, c + dc\n",
        "                        if not self.in_bounds(new_r, new_c):\n",
        "                           #if move would take it off the grid stay in place\n",
        "                           new_r, new_c = r, c\n",
        "                        q = self.get_reward(r, c) + gamma * V[(new_r, new_c)]\n",
        "\n",
        "                        if q > best:\n",
        "                          best = q\n",
        "\n",
        "                     newV[s] = best\n",
        "                     if abs(best - V[s]) > delta:\n",
        "                      delta = abs(best - V[s])\n",
        "\n",
        "              V = newV\n",
        "              self._print_values(V)\n",
        "              if delta < theta:\n",
        "                 break\n",
        "\n",
        "         return [[V[(r, c)] for c in range(self.ncols)] for r in range(self.nrows)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb15497-6f08-4f3e-abef-43099f05fba7",
      "metadata": {
        "id": "dfb15497-6f08-4f3e-abef-43099f05fba7"
      },
      "source": [
        "#### 2) Run value iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "922da11f-c12f-4535-8a8a-63cbcab3e736",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "922da11f-c12f-4535-8a8a-63cbcab3e736",
        "outputId": "efdc79a9-bfff-4e7c-c7e2-8aa48c68003a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0.000  0.000  0.000\n",
            " 0.000  0.000  0.000\n",
            " 0.000  0.000  0.900\n",
            " 0.000  0.900  1.000\n",
            "\n",
            " 0.000  0.000  0.000\n",
            " 0.000  0.000  0.810\n",
            " 0.000  0.810  0.900\n",
            " 0.810  0.900  1.000\n",
            "\n",
            " 0.000  0.000  0.729\n",
            " 0.000  0.729  0.810\n",
            " 0.729  0.810  0.900\n",
            " 0.810  0.900  1.000\n",
            "\n",
            " 0.000  0.656  0.729\n",
            " 0.656  0.729  0.810\n",
            " 0.729  0.810  0.900\n",
            " 0.810  0.900  1.000\n",
            "\n",
            " 0.590  0.656  0.729\n",
            " 0.656  0.729  0.810\n",
            " 0.729  0.810  0.900\n",
            " 0.810  0.900  1.000\n",
            "\n",
            " 0.590  0.656  0.729\n",
            " 0.656  0.729  0.810\n",
            " 0.729  0.810  0.900\n",
            " 0.810  0.900  1.000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gw = GridWorld()\n",
        "V_final = gw.value_iteration()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486efc92-9a71-4956-b877-b6b7cef13031",
      "metadata": {
        "id": "486efc92-9a71-4956-b877-b6b7cef13031"
      },
      "source": [
        "#### 3) **(POINTS: 2)** Based on the value function: After the agent has moved right or down, does it ever make sense for it to backtrack (move up or left)? Explain your reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f4d310a-493c-4bcc-8328-c09c88116b0e",
      "metadata": {
        "id": "5f4d310a-493c-4bcc-8328-c09c88116b0e"
      },
      "source": [
        "No, every move right or left increases the value function, so it does not pay to go backwards. Also extra steps would further decrease the value because of additional discounting at the additional steps."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EQCs-cl32hWu"
      },
      "id": "EQCs-cl32hWu"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oeiVOHaP2hTA"
      },
      "id": "oeiVOHaP2hTA"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}